1. What is the authoritative **allowed unit dictionary** for MVP (metric + US), including synonyms and pluralization (e.g., g/gram/grams), and what is the normalization strategy?

Recommendation: Define a versioned **unit registry** (JSON) with canonical units + synonyms per system; normalize during parse; log unknown units and map the most common ones weekly.

2. How do we represent and store **“non-scalable markers”** (can/pack/to taste) in the ingredient schema so scaling and calorie estimation can reliably ignore them?

Recommendation: Add a boolean + reason field per ingredient line: **is_scalable**, **non_scalable_reason**, and keep parsed_quantity/unit nullable; scaling step leaves raw_text unchanged and adds “(not scaled)” label.

3. What is the MVP rule for **ingredient line splitting/merging** (e.g., “salt and pepper” or “1 onion, chopped”): do we require users to fix these, or do we accept partial parsing?

Recommendation: Accept partial parsing but enforce the gate: if a line contains multiple items, mark it **low confidence** and require edit only if it blocks reaching the ≥80% parse threshold.

4. What are the precise **fallback behaviors** for the 8s timeout: do we return scaled-only immediately at 8s, or do we stream/step UI (scaled first, then rewrite if it finishes)?

Recommendation: Implement a two-phase response: always return **scaled output ≤2s**; attempt rewrite+estimate in background of the same request until 8s; if not done, finalize as scaled-only + “Calories unavailable” + “Try again” CTA.

5. What are the product rules around **target calories per serving** when “stays similar” constraints prevent meeting the target—how do we message this without a comparison view?

Recommendation: Add a short standardized note in metadata: **“Target not achievable within selected aggressiveness/keep-similar; closest estimate shown.”** No diff view, but clear explanation.

6. How do we determine and persist **primary protein + core flavor base** from heuristics: do we store the detected items as metadata on the recipe version for transparency/debugging?

Recommendation: Store **detected_core_items[]** (protein + flavor base) in version metadata and use it to enforce “never remove”; expose only via internal logs in MVP, not UI.

7. What are the rules for **user avoid list** interactions with keep-similar (e.g., avoid = “chicken,” but chicken is primary protein): which constraint wins?

Recommendation: Make avoid list a hard constraint: if avoid conflicts with core items, require user choice: **disable keep-similar** or accept alternative protein; otherwise tailoring proceeds but must not include avoided items.

8. What is the MVP behavior for **allergens/diet labels** (none required, but avoid list exists): do we attempt to infer hidden ingredients (e.g., butter in “sauce”), or only match explicit ingredient names?

Recommendation: MVP = explicit matching only against parsed_item + raw_text; do not infer hidden ingredients; label it clearly in settings/help as “avoid list matches listed ingredients.”

9. What are the exact **data retention** and deletion semantics for versions and events: when a recipe is deleted, do we delete all versions and related events, or keep aggregated metrics?

Recommendation: Delete recipe + all versions; keep only non-identifiable aggregated metrics if needed (or delete all user-linked events if strict privacy). Document one policy and implement it consistently.

10. What is the MVP scope for **device/responsive UX** (mobile first vs desktop), especially for the copy/paste + raw line editing experience?

Recommendation: Prioritize mobile-friendly editing: ingredient lines as a vertical list with inline edit; ensure “paste → auto-format → edit lines → run step 1” fits without horizontal scrolling; define responsive acceptance criteria in PRD.


------------------------------------ Answers -----------------------------------------

1. Define a versioned **unit registry** (JSON) with canonical units + synonyms per system; normalize during parse; log unknown units and map the most common ones weekly.
2. Add a boolean + reason field per ingredient line: **is_scalable**, **non_scalable_reason**, and keep parsed_quantity/unit nullable; scaling step leaves raw_text unchanged and adds “(not scaled)” label.
3. Accept partial parsing but enforce the gate: if a line contains multiple items, mark it **low confidence** and require edit only if it blocks reaching the ≥80% parse threshold.
4. Implement a two-phase response: always return **scaled output ≤2s**; attempt rewrite+estimate in background of the same request until 8s; if not done, finalize as scaled-only + “Calories unavailable” + “Try again” CTA.
5. Add a short standardized note in metadata: **“Target not achievable within selected aggressiveness/keep-similar; closest estimate shown.”** No diff view, but clear explanation.
6. Store **detected_core_items[]** (protein + flavor base) in version metadata and use it to enforce “never remove”; expose only via internal logs in MVP, not UI.
7. Make avoid list a hard constraint: if avoid conflicts with core items, require user choice: **disable keep-similar** or accept alternative protein; otherwise tailoring proceeds but must not include avoided items.
8. MVP = explicit matching only against parsed_item + raw_text; do not infer hidden ingredients; label it clearly in settings/help as “avoid list matches listed ingredients.”
9. Delete recipe + all versions; keep only non-identifiable aggregated metrics if needed (or delete all user-linked events if strict privacy). Document one policy and implement it consistently.
10. Prioritize mobile-friendly editing: ingredient lines as a vertical list with inline edit; ensure “paste → auto-format → edit lines → run step 1” fits without horizontal scrolling; define responsive acceptance criteria in PRD.

---

You are an AI assistant whose task is to summarize a conversation about PRD (Product Requirements Document) planning for MVP and prepare a concise summary for the next development stage. In the conversation history you will find the following information:
1. Project description
2. Identified user problem
3. Conversation history containing questions and answers
4. Recommendations regarding PRD content

Your task is to:
1. Summarize the conversation history, focusing on all decisions related to PRD planning.
2. Match the model's recommendations to the answers given in the conversation history. Identify which recommendations are relevant based on the discussion.
3. Prepare a detailed conversation summary that includes:
   a. Main functional requirements of the product
   b. Key user stories and usage paths
   c. Important success criteria and ways to measure them
   d. Any unresolved issues or areas requiring further clarification
4. Format the results as follows:

<conversation_summary>
<decisions>
[List decisions made by the user, numbered].
</decisions>

<matched_recommendations>
[List of the most relevant recommendations matched to the conversation, numbered]
</matched_recommendations>

<prd_planning_summary>
[Provide a detailed summary of the conversation, including the elements listed in step 3].
</prd_planning_summary>

<unresolved_issues>
[List any unresolved issues or areas requiring further clarification, if any exist]
</unresolved_issues>
</conversation_summary>

The final result should contain only content in markdown format. Ensure that your summary is clear, concise, and provides valuable information for the next stage of creating the PRD.
